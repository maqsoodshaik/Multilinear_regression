{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** For all implementations in this assignment, make sure to use PyTorch whenever possible. Most computations on vectors and matrices can be implemented very efficiently using the PyTorch API. There is no need for looping over vectors etc using `for` loops. As a simple example, in order to compute the mean of a vector, just use `torch.mean()`. If you are not familiar with PyTorch please consult the PyTorch tutorials online. Further, in case of any doubts, the Piazza forum is the best place to ask questions and clarifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##  Multiple Linear Regression\n",
    "\n",
    "In this exercise you will learn about [*multiple linear regression*](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression) while also experimenting with hyperparameter tuning. Performing regression on one independent (or explanatory) variable and a scalar dependent variable is called **simple linear regression**.\n",
    "But, when there are more than one explanatory variable (i.e. $x^{(1)}, x^{(2)}, ...,x^{(k)}$), and a single scalar dependent variable (*y*), then it's called **multiple linear regression**. (Do not confuse this with *multivariate linear regression* where we predict more than one (correlated) dependent variable.)\n",
    "\n",
    "Here, you will be implementing a **multiple linear regression** model in Python/PyTorch using the [*vanilla Gradient Descent*](https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants) algorithm. Particularly, we will be using a variant of the **stochastic gradient descent** (*SGD*) where one performs the update step using a small set of training samples of size *batch_size* which we will set to 64, i.e. we go through the training samples, sampling 64 at a time, and perform gradient descent. Such a procedure is sometimes called as **mini-batch gradient descent** in the deep neural networks community.\n",
    "\n",
    "Going through all the training samples *once* is called an **epoch**. Ideally, the training procedure has to go through multiple epochs over the training samples, each time shuffling it, until a convergence criterion has been satisfied. Here, we will set a *tolerance value* for the difference in error (i.e. change in Mean Squared Error (MSE) values between subsequent epochs) that we will accept. Once this difference falls below the *tolerance value*, we terminate our training phase and return the latest parameters. \n",
    "\n",
    "We repeat the above training procedure for all possible hyperparameter combinations in order to find the best parameters (i.e. weights) for our model. For this so called *hyperparameter tuning* we will be using the validation data. \n",
    "\n",
    "As a next step, we will combine training data and validation data and make it as our *new training data*. We keep the test data as it is, untouched throughout our experiments. Using the hyperparameter combination (for the least MSE) that we found above, we train the model *again* with the *new training data* and obtain the parameter (*i.e. weight vector*) after convergence according to our *tolerance value*.\n",
    "\n",
    "Phew! That will be our much desired *weight vector*. This is then used on the *test data*, which has not been seen by our algorithm so far, to make a prediction. The resulting MSE value will be the so-called [*generalization error*](https://en.wikipedia.org/wiki/Generalization_error). It is this *generalization error* that we want it to be as low as possible for some *unseen data* (implies that we can achieve higher accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Dataset\n",
    "For our task, we will be using the *Wine Quality* dataset and predict the quality of white wine based on 11 features such as acidity, citric acid content, residual sugar etc. . You can take a glance of the data using functions like *data.head()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 12)\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "data = pd.read_csv(data_url, sep=';')\n",
    "\n",
    "# inspect data\n",
    "display(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "# Get data as NumPy array\n",
    "data_np = data.values\n",
    "\n",
    "# Convert the data to Torch tensor\n",
    "data_tensor = torch.tensor(data_np, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "####  Loss function\n",
    "We will use a *regularized* form of the MSE loss function. In matrix-vector format it can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "    J(\\textbf{w}) = \\frac{1}{2} \\Vert{X\\textbf{w}-\\textbf{y}}\\Vert^{2} + \\frac{\\lambda}{2}\\Vert{\\textbf{w}}\\Vert^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "It's important to note that, in the above equation, $X$, called [**design matrix**](https://en.wikipedia.org/wiki/Design_matrix#Definition), consists of data points in our dataset. Each row corresponds to a data point whereas each column represents a feature. Therefore, the dimension of $X$ is *(number of data points, number of features)*. $X$ can be also thought as of the vertical concatenation of data points of shape *(batch_size, num_features)*. To make things easier and computationally efficient, you can add the *bias* term as the first column of $X$. Take care to have the *weight* vector $\\textbf{w}$ with matching dimensions. <br > (Hint: see [Design_matrix#Multiple_regression](https://en.wikipedia.org/wiki/Design_matrix#Multiple_regression) for how $X$ with 2 features looks like for a $1^{st}$ degree polynomial.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Derive the gradient (w.r.t $\\textbf{w}$) for the regularized objective given in 4.3.2. \\[**Hint**: You can use your results from 3.2(a). However, make sure to adapt the result to the above objective\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Matrix-vector format for higher order polynomial\n",
    "\n",
    "Written in matrix form, a linear regression model for second-order would look like: <br />\n",
    "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + \\textbf{b}$$\n",
    "\n",
    "where $X^{2}$ is the element-wise squaring (i.e., Hadamard product of X with itself) of the original design matrix $X$, $\\textbf{w}_1$ and $\\textbf{w}_2$ are the *weight* vectors, and **b** is the *bias* vector.\n",
    "\n",
    "**Task 2:** (0.25 point) <br >\n",
    "Write down the matrix-vector format for an $8^{th}$ order linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Hyperparameters\n",
    "Next, we will experiment with three hyperparameters for developing our model:\n",
    "\n",
    "i) regularization parameter $\\lambda$ <br />\n",
    "ii) learning rate $\\epsilon$ <br />\n",
    "iii) order of polynomial *p*\n",
    "\n",
    "And do a grid search over the values that these hyperparameters can take in order to select the best combination (i.e. the one that will achieve the lowest *test* error on our data). This approach is called **hyperparameter optimization or tuning**. For convenience and computational reasons, we will experiment with only three values for each of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fix possible hyperparameters\n",
    "polynomial_orders = [1, 5, 9]\n",
    "learning_rates = [1e-5, 1e-6, 1e-8]\n",
    "lambdas = [0.1, 0.5, 0.8]\n",
    "\n",
    "# Fix batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Normalization\n",
    "First of all, inspect the data, and understand its structure and features. Ideally, before starting to train our learning algorithm, we would want the data to be normalized. Here, we normalize the data (i.e. normalize each column) using the following formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "  norm\\_x_i = \\frac{x_i - min(x)}{max(x) - min(x)}\n",
    "\\end{equation*}\n",
    "where $x_i$ is the $i^{th}$ sample in feature $x$.\n",
    "\n",
    "**Task 3:** (0.25 point) <br > \n",
    "Complete the following function which performs normalization (i.e. normalizes the columns of $X$). Use only PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(data):\n",
    "    # TODO: implement\n",
    "    data = (data-data.min(0,keepdim = True)[0])/(data.max(0,keepdim = True)[0]-data.min(0,keepdim = True)[0])\n",
    "    return data\n",
    "\n",
    "\n",
    "# Perform data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.25$\n",
    "\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting and Shuffling\n",
    "Typically, we need to divide our data into 3 splits \\[ train (80%), validation (10%), and test (10%)\\] for experimentation purposes. And shuffle the training data during every epoch.\n",
    "\n",
    "\n",
    "Implement the following function `split_data()`. You can either implement it manually using `torch` or use sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "def split_data(data, n_train=3898, n_val=500, n_test=500):\n",
    "    # TODO: implement\n",
    "    train_n = (n_train+n_val)/(n_train+n_val+n_test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[:,:-1],data[:,-1], test_size=(1-train_n), random_state=1)\n",
    "    val_n = n_val/n_train\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_n, random_state=1) \n",
    "    return X_train, X_val, X_test,y_train, y_val,y_test\n",
    "\n",
    "\n",
    "# Shuffle only the training data along axis 0\n",
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    # shuffling of data along axis 0\n",
    "    X_train, Y_train = shuffle(X_train, Y_train)\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of required functions\n",
    "\n",
    "\n",
    "Complete the following function which computes the MSE value. You can ignore the regularization term and also the constant $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Mean Squared Error \n",
    "def compute_mse(prediction, ground_truth):\n",
    "    '''\n",
    "    :param prediction: a nx1 vector represents the prediciton of your model\n",
    "    :param ground_truth: a nx1 vector represents the ground_truth\n",
    "    :return: MSE loss\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    mse =  torch.mean((torch.square(prediction - ground_truth)))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Implement the function which computes the prediction of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction(X, W):\n",
    "    '''\n",
    "    Given a design matrix X (could be a batch) and parameters W, calculate the prediction Yhat.\n",
    "    :param X: desgin matrix X of dimension nxk, where n is the number of data points (in the batch).\n",
    "    :param W: parameters\n",
    "    :return Yhat: the predictions\n",
    "    '''\n",
    "    # TODO: implement\n",
    "\n",
    "    prediction = torch.matmul(X.float(),W)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function which computes the gradient of your loss function. That is, implement the gradient arrived at in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, Yhat, W, lambda_):\n",
    "    '''\n",
    "    :param X: designmatrix X\n",
    "    :param Y: ground truth labels correspoinding to X\n",
    "    :param Yhat: predicted labels\n",
    "    :param W: parameters\n",
    "    :param lambda_: coefficient for the regularizer\n",
    "    :return: gradient w.r.t W\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    gradient = torch.matmul(torch.transpose(X,0,1).float(),Yhat.float())+lambda_*(W)-torch.matmul(torch.transpose(X,0,1).float(),Y.float())\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function which performs a single update step of mini-batch GD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hint: avoid in-place modification\n",
    "def sgd(gradient, lr, cur_W):\n",
    "    '''\n",
    "    :param gradient: gradient at cur_W\n",
    "    :param lr: learning rate\n",
    "    :param cur_W: current value of parameters\n",
    "    :return new_W: perform parameter update (using gradient descent) and return new_W\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    new_W = cur_W-lr*gradient\n",
    "    return new_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Complete the following function which reformats your data as a design matrix, and accordingly the weight vector, for a given polynomial order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concatenate X acc. to order of polynomial; likewise do it for W\n",
    "# where X is design matrix, W is the corresponding weight vector\n",
    "# concatenate matrix X horizontally, concatenate W vertically. That is, after this transformation, X gets broader and W gets longer\n",
    "# e.g. [1 X X^2 X^3], [1 W1 W2 W3]   #the square brackets does not signify python lists\n",
    "# \n",
    "def prepare_data_matrix(X, W, order,b,i):\n",
    "    # TODO: implement\n",
    "    X_old = X\n",
    "    W_old = W\n",
    "    X = torch.cat((torch.ones(X.size()[0],1),X),1)\n",
    "    if(b ==0 and i == 0):\n",
    "        W = torch.cat((torch.ones(1,1),W),0)\n",
    "        \n",
    "    for j in range(order-1):\n",
    "        X = torch.cat((X,X_old**(j+2)), 1)\n",
    "        if(b ==0 and i == 0):\n",
    "    \n",
    "            W = torch.cat((W,W_old), 0)\n",
    "            \n",
    "    return X,W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Training\n",
    "\n",
    "\n",
    "Complete the code in the following cell such that it performs **mini-batch gradient descent** on the training data for all possible hyperparameter combinations.\n",
    "\n",
    "Note: You can also define a function, named appropriately (e.g. `train()`), which performs training. And, take care to do correct bookkeeping of hyperparameter combinations, weight vectors, and the MSE values in your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, W_init, polynomial_orders, learning_rates, lambdas,i):\n",
    "    # TODO: implement\n",
    "    batch = int(X_train.shape[0]/64)\n",
    "    Y_train = torch.reshape(Y_train,(Y_train.size()[0],1))\n",
    "    W = W_init\n",
    "    loss =0\n",
    "    for b in range(batch):#mini-batch of 64\n",
    "        X = X_train[64*b:64*(b+1),:]\n",
    "        Y = Y_train[64*b:64*(b+1),:]\n",
    "        X,W= prepare_data_matrix(X, W, polynomial_orders,b,i)\n",
    "        Yhat = get_prediction(X,W)\n",
    "        grad = compute_gradient(X, Y, Yhat, W, lambdas)\n",
    "        W = sgd(grad, learning_rates, W)\n",
    "    X = X_train[64*(b+1):,:]\n",
    "    Y = Y_train[64*(b+1):,:]\n",
    "    X,W= prepare_data_matrix(X, W, polynomial_orders,b,i)\n",
    "    Yhat = get_prediction(X,W)\n",
    "    loss = compute_mse(Yhat, Y)\n",
    "    grad = compute_gradient(X, Y, Yhat, W, lambdas)\n",
    "    W = sgd(grad, learning_rates, W)\n",
    "    return W,loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Complete the following function which selects the best hyperparameter combination given a set of weights (i.e. the one that gives lowest MSE on **validation data**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select hparams of minimum MSE on Validation data\n",
    "def validation(X_val, W,y_val,order):\n",
    "    # TODO: Implement\n",
    "    X_val, W = prepare_data_matrix(X_val, W, order,b=1,i=1)\n",
    "    y_val = torch.reshape(y_val,(y_val.size()[0],1))\n",
    "    pred = get_prediction(X_val, W)\n",
    "    mse = compute_mse(pred, y_val)\n",
    "    return mse\n",
    "\n",
    "def select_best_hparams(mse):\n",
    "    best = mse.index(min(mse))#returns index of the best possible hyperparameters using mse as reference\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train the model for all possible hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polynomial_order: 1,learning_rate: 1e-05,lambda: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Train the model with all possible hyperparameter combinations\n",
    "\n",
    "\n",
    "# Find best hyperparameter combination\n",
    "data = data_normalization(data_tensor)\n",
    "X_train, X_val, X_test,y_train, y_val,y_test=split_data(data, n_train=3898, n_val=500, n_test=500)\n",
    "W_init = torch.rand(X_train.size()[1],1)#initializing weights with random values\n",
    "best_ar = []\n",
    "l_ar =[]\n",
    "j_ar = []\n",
    "k_ar = []\n",
    "for j in range(3):#loops for traversing through all possible combinations of hyperparameters\n",
    "    \n",
    "    for k in range(3):\n",
    "        \n",
    "        for l in range(3):\n",
    "            W = W_init\n",
    "            X_train, y_train = shuffle_train_data(X_train, y_train)#shuffling before every epoch\n",
    "            loss_i  = 0\n",
    "            for i in range(200): \n",
    "                W,loss = train(X_train, y_train, W, polynomial_orders[j], learning_rates[k], lambdas[l],i)\n",
    "                if (abs(loss_i-loss)<0.01):\n",
    "                    break#tolerance value of 0.01\n",
    "                loss_i = loss\n",
    "            mse = validation(X_val, W,y_val,polynomial_orders[j])\n",
    "            best_ar.append(mse)\n",
    "            l_ar.append(l)\n",
    "            j_ar.append(j)\n",
    "            k_ar.append(k)\n",
    "best= select_best_hparams(best_ar)\n",
    "print(f'polynomial_order: {polynomial_orders[j_ar[best]]},learning_rate: {learning_rates[k_ar[best]]},lambda: {lambdas[l_ar[best]]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Re-Training on Train + Validation data\n",
    "Now, we will concatenate the training and validation data and make it as the new training data.\n",
    "Complete the following which does re-training on the combined training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the training on X_train + X_val combined\n",
    "\n",
    "# TODO: implement\n",
    "X_train = torch.cat((X_train,X_val), 0)\n",
    "y_train = torch.cat((y_train,y_val), 0)\n",
    "W = W_init\n",
    "loss_ar = []\n",
    "for i in range(200): \n",
    "    W,loss = train(X_train, y_train, W, polynomial_orders[j_ar[best]], learning_rates[k_ar[best]], lambdas[l_ar[best]],i)\n",
    "    loss_ar.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Visualizing MSE over epochs:** \n",
    "\n",
    "\n",
    "Plot the MSE values (y-axis) against epochs (x-axis) using matplotlib.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training loss vs Epochs')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot the convergence of MSE values using matplotlib, i.e. #epochs on X-axis and MSE values on Y-axis\n",
    "# TODO: implement\n",
    "fig, axes = plt.subplots()\n",
    "plt.plot(range(len(loss_ar)),loss_ar)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.title(\"Training loss vs Epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Evaluation on Test set\n",
    "\n",
    "Evaluate your model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mse loss for test set:0.02774468296341164\n"
     ]
    }
   ],
   "source": [
    "# test your model on X_test with the weight vector that you found above\n",
    "# this will be the generalization error of our model.\n",
    "\n",
    "# TODO: implement\n",
    "mse_test = validation(X_test, W,y_test,polynomial_orders[j_ar[best]])\n",
    "print(f'Mse loss for test set:{mse_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Results\n",
    "\n",
    "Report the MSE value on the test data. Which hyperparameter combination turned out to be the best? In your understanding, why do you think such a combination turned out to be the best for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Comments:** \n",
    "\n",
    "1.MSE value on the test data is around 0.02\n",
    "\n",
    "2.Best hyperparameters of polynomial_order: 1,learning_rate: 1e-05 and lambda: 0.8\n",
    "\n",
    "3.As observed with increasing polynomial order the epochs needed to learn increases as number of weight parameters increases with polynomial order, since we limited our epochs to only 200, polynomial with order 1 performs best. \n",
    "when it comes to learning rate it took the biggest learning rate possible among provided ones. also for lambda it took the biggest possible value in provided ones as it regulizes the loss in the correct amount.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
